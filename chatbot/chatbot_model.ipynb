{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# things we need for NLP\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# things we need for Tensorflow\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'intents': [{u'patterns': [u'Tere', u'Hei', u'Tsau', u'Tsauki'], u'tag': u'name', u'responses': [u'Tere, mis su nimi on? \\U0001f600\\U0001f600\\U0001f600'], u'context_set': u'name'}, {u'responses': [u'Ole n\\xfc\\xfcd, saame tuttavaks, vana sa oled? \\U0001f600', u'\\xc4ra karda, ma tunnen su vanemaid, kui vana sa oled? \\U0001f600'], u'positive': False, u'patterns': [u'Ei \\xfctle', u'Ei'], u'tag': u'age_neg', u'context_filter': u'name', u'context_set': u'age'}, {u'responses': [u'R\\xf5\\xf5m tutvuda, ma olen Mari ja olen 14-aastane. Kui vana sa oled?', u'\\U0001f600 Tore! Ma olen Martin, 13. Vana sa?', u'Ma olen Kadri, kohe saan 12. Kui vana sa praegu oled?'], u'positive': True, u'patterns': [u'Olen', u'Minu nimi', u' '], u'tag': u'age_pos', u'context_filter': u'name', u'context_set': u'age'}, {u'responses': [u'Pole hullu, k\\xfcll sa varsti usaldad mind! Mul on kodus koer ja me elame perega Viimsis suures majas. Kus sa elad?', u'Vanus on k\\xf5igest number \\U0001f600. Mul on kodus kass ja ma elan \\xfches korteris Mustam\\xe4el. Kus sa elad?'], u'positive': False, u'patterns': [u'Ei \\xfctle', u'Ei', u'mkm'], u'tag': u'home_neg', u'context_filter': u'age', u'context_set': u'home'}, {u'responses': [u'Juba nii vana! Kas sa m\\xe4letad mind? Mul on kodus koer ja me elame perega Viimsis suures majas. Kus sa elad?', u'Juba varsti t\\xe4iskasvanud! Tutvustan ennast! Mul on kodus kass ja ma elan \\xfches korteris Mustam\\xe4el. Kus sa elad?'], u'positive': True, u'patterns': [u'Olen', u'Vana', u'Aastat', u' ', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'7', u'8', u'9'], u'tag': u'home_pos', u'context_filter': u'age', u'context_set': u'home'}, {u'responses': [u'\\xc4ra muretse, tean kus sa oled. Kas su vanemad on kodus?', u'Olgu, aga kus su vanemad on kodus?'], u'positive': False, u'patterns': [u'Ei \\xfctle', u'Ei', u'mkm'], u'tag': u'location_neg', u'context_filter': u'home', u'context_set': u'parents'}, {u'responses': [u'V\\xe4ga huvitav! Muideks, kas sul vanemad on kodus?', u'Kas oskad mulle \\xf6elda, kas su vanemad on kodus?'], u'positive': True, u'patterns': [u'Elan', u'Majas', u'Korteris', u'S\\xf5bra'], u'tag': u'location_pos', u'context_filter': u'home', u'context_set': u'parents'}, {u'responses': [u'Okei Okei \\U0001f602 \\xc4ra karda, r\\xe4\\xe4gime lihtsalt. Kas sul ka lemmikloomi on?', u'Olgu, \\U0001f602 \\xc4ra karda, r\\xe4\\xe4gime lihtsalt, kuule, kas sul ka m\\xf5ni lemmikloom on?'], u'positive': False, u'patterns': [u'Ei \\xfctle'], u'tag': u'pets_neg', u'context_filter': u'parents', u'context_set': u'pet'}, {u'responses': [u'Okei \\U0001f602 Kas sul ka lemmikloomi on?', u'Olgu, \\U0001f602 kuule, kas sul ka m\\xf5ni lemmikloom on?'], u'positive': True, u'patterns': [u'Pole', u'Mujal', u'T\\xf6\\xf6l', u'Poes', u'Ei ole', u'siin', u'Jah', u'On', u' '], u'tag': u'pets_pos', u'context_filter': u'parents', u'context_set': u'pet'}, {u'responses': [u'Minul on \\xfcks koer. Tead, mul on \\xfcks h\\xe4sti lahe YouTube\\u2019i konto, ma panen sinna iga n\\xe4dal \\xfche video \\xfcles. Kas sul ka seal konto on? Anna oma kasutajanimi ja ma Subscriben!'], u'positive': False, u'patterns': [u'Ei ole', u'Ei \\xfctle', u'Mkm'], u'tag': u'youtube_neg', u'context_filter': u'pet', u'context_set': u'youtube'}, {u'responses': [u'V\\xe4ga huvitav. Tead, mul on \\xfcks h\\xe4sti lahe YouTube\\u2019i konto, ma panen sinna iga n\\xe4dal \\xfche video \\xfcles. Kas sul ka seal konto on? Anna oma kasutajanimi ja ma Subscriben!'], u'positive': True, u'patterns': [u'On k\\xfcll', u'Jah', u'Kass', u'Koer', u'Hamster', u'Hiir', u'Merisiga', u' '], u'tag': u'youtube_pos', u'context_filter': u'pet', u'context_set': u'youtube'}, {u'responses': [u'Okei, v\\xf5iksid endast videosid teha! Kuule, mis su Facebook on? Ma tahaks sind s\\xf5braks lisada!'], u'positive': False, u'patterns': [u'Ei \\xfctle', u'Sinu asi', u'Ei vaja'], u'tag': u'subscribe_neg', u'context_filter': u'youtube', u'context_set': u'facebook'}, {u'responses': [u\"Okei, Subscribe'in! Kuule, mis su Facebook on? Ma tahaks sind s\\xf5braks lisada!\"], u'positive': True, u'patterns': [u'On k\\xfcll', u'Ei ole', u'Ole', u'Tahaks', u'Minu', u' ', u'On', u'Jah', u'Ja', u'Jh', u'Ikka'], u'tag': u'subscribe_pos', u'context_filter': u'youtube', u'context_set': u'facebook'}, {u'responses': [u'Ok, kunagi v\\xf5iksid teha \\U0001f602 Kuule, mis klassis sa k\\xe4isidki?'], u'positive': False, u'patterns': [u'Ei ole', u'Ei \\xfctle', u'Pole', u'mkm'], u'tag': u'facebook_neg', u'context_filter': u'facebook', u'context_set': u'class'}, {u'responses': [u'Ok, lisan sind s\\xf5braks \\U0001f602 Kuule, mis klassis sa k\\xe4isidki?'], u'positive': True, u'patterns': [u'Minu', u'Jah', u'On', u'See', u' '], u'tag': u'facebook_pos', u'context_filter': u'facebook', u'context_set': u'class'}, {u'patterns': [u'Ei \\xfctle', u'Ei', u'Pole'], u'tag': u'grade_neg', u'context_filter': u'class', u'responses': [u'Ok, mis teha \\U0001f602 ! Kuule, ma pean praegu minema, r\\xe4\\xe4gime teine kord uuesti! Tsau!'], u'positive': False}, {u'patterns': [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'\\xdclikool', u'Baka', u'lasteaed', u'eelkool'], u'tag': u'grade_pos', u'context_filter': u'class', u'responses': [u'T\\xe4nan vestlemast \\U0001f602 ! Kuule, ma pean praegu minema, r\\xe4\\xe4gime teine kord uuesti! Tsau!'], u'positive': True}, {u'patterns': [u'Head aega!', u'N\\xe4gemist!', u'Tsau!'], u'tag': u'goodbye', u'context_filter': u'bye', u'responses': [u'N\\xe4gemist']}]}\n"
     ]
    }
   ],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "    \n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'documents')\n",
      "(18, 'classes', [u'age_neg', u'age_pos', u'facebook_neg', u'facebook_pos', u'goodbye', u'grade_neg', u'grade_pos', u'home_neg', u'home_pos', u'location_neg', u'location_pos', u'name', u'pets_neg', u'pets_pos', u'subscribe_neg', u'subscribe_pos', u'youtube_neg', u'youtube_pos'])\n",
      "(62, 'unique stemmed words', [u'!', u'1', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'aast', u'aeg', u'as', u'bak', u'eelkool', u'ei', u'el', u'hamst', u'head', u'hei', u'hiir', u'ikk', u'ja', u'jah', u'jh', u'kass', u'koer', u'kort', u'k\\xfcll', u'lastea', u'maja', u'merisig', u'minu', u'mkm', u'muj', u'nim', u'n\\xe4gemist', u'ol', u'on', u'poe', u'pol', u'see', u'siin', u'sinu', u's\\xf5bra', u'tahak', u'ter', u'tsau', u'tsauk', u't\\xf6\\xf6l', u'vaj', u'van', u'\\xfclikool', u'\\xfctle'])\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12999  | total loss: \u001b[1m\u001b[32m0.82870\u001b[0m\u001b[0m | time: 0.037s\n",
      "| Adam | epoch: 1000 | loss: 0.82870 - acc: 0.6062 -- iter: 096/100\n",
      "Training Step: 13000  | total loss: \u001b[1m\u001b[32m0.82756\u001b[0m\u001b[0m | time: 0.040s\n",
      "| Adam | epoch: 1000 | loss: 0.82756 - acc: 0.6206 -- iter: 100/100\n",
      "--\n",
      "INFO:tensorflow:/home/salts/cybersecurity-exhibition/chatbot/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[u'age_neg', u'age_pos', u'facebook_neg', u'facebook_pos', u'goodbye', u'grade_neg', u'grade_pos', u'home_neg', u'home_pos', u'location_neg', u'location_pos', u'name', u'pets_neg', u'pets_pos', u'subscribe_neg', u'subscribe_pos', u'youtube_neg', u'youtube_pos']\n"
     ]
    }
   ],
   "source": [
    "p = bow(\"is your shop open today?\", words)\n",
    "print (p)\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2318924e-05 1.3873255e-01 8.1763286e-03 1.7084652e-01 2.5544237e-08\n",
      "  2.2495251e-04 1.4882818e-02 4.9710219e-05 9.1532141e-02 5.5388999e-05\n",
      "  1.8111272e-05 6.1881146e-04 1.0003788e-05 1.0571235e-01 2.3274424e-06\n",
      "  2.8498334e-01 8.1595788e-03 1.7598271e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
